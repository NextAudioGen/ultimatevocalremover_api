{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEN2Ei3D284H"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nkv0g0H0yNA4"
      },
      "outputs": [],
      "source": [
        "from mdx.tfc_tdf_v3 import TFC_TDF_net, STFT\n",
        "from mdx.tfc_tdf_v3 import TFC_TDF_net, STFT\n",
        "import mdx.mdxnet as MdxnetSet\n",
        "from mdx import spec_utils\n",
        "from mdx.constants import secondary_stem\n",
        "import onnxruntime as ort\n",
        "from onnx import load\n",
        "from onnx2pytorch import ConvertModel\n",
        "from ml_collections import ConfigDict\n",
        "\n",
        "import torch\n",
        "import audiofile\n",
        "from IPython.display import Audio, display\n",
        "import soundfile as sf\n",
        "import json \n",
        "import hashlib\n",
        "import librosa\n",
        "import numpy as np\n",
        "import audioread\n",
        "import platform\n",
        "from numpy.typing import NDArray\n",
        "from typing import Union\n",
        "import math, os\n",
        "import yaml\n",
        "\n",
        "if torch.cuda.is_available(): device = \"cuda\"\n",
        "elif torch.backends.mps.is_available(): device = torch.device(\"mps\")\n",
        "else: device = \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhWOPekI3IbK"
      },
      "source": [
        "# Main code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load model config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "audio:\n",
              "  chunk_size: 261120\n",
              "  dim_f: 4096\n",
              "  dim_t: 256\n",
              "  hop_length: 1024\n",
              "  min_mean_abs: 0.001\n",
              "  n_fft: 8192\n",
              "  num_channels: 2\n",
              "  sample_rate: 44100\n",
              "inference:\n",
              "  batch_size: 1\n",
              "  dim_t: 256\n",
              "  num_overlap: 8\n",
              "model:\n",
              "  act: gelu\n",
              "  bottleneck_factor: 4\n",
              "  growth: 128\n",
              "  norm: InstanceNorm\n",
              "  num_blocks_per_scale: 2\n",
              "  num_channels: 128\n",
              "  num_scales: 5\n",
              "  num_subbands: 4\n",
              "  scale:\n",
              "  - 2\n",
              "  - 2\n",
              "training:\n",
              "  augmentation: 1\n",
              "  augmentation_mix: true\n",
              "  augmentation_type: simple1\n",
              "  batch_size: 6\n",
              "  coarse_loss_clip: true\n",
              "  ema_momentum: 0.999\n",
              "  grad_clip: 0\n",
              "  instruments:\n",
              "  - Vocals\n",
              "  - Instrumental\n",
              "  lr: 1.0e-05\n",
              "  num_epochs: 1000\n",
              "  num_steps: 1000\n",
              "  patience: 2\n",
              "  q: 0.95\n",
              "  reduce_factor: 0.95\n",
              "  target_instrument: null"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "def load_mdxc_models_data(model_path:str=\"mdxc/modelparams/model_data.json\")->dict:\n",
        "    \"\"\"\n",
        "    Load the mdxc models data from the specified model path.\n",
        "\n",
        "    Args:\n",
        "        model_path (str): The path to the model data JSON file. Default is \"mdxc/modelparams/model_data.json\".\n",
        "\n",
        "    Returns:\n",
        "        dict: The loaded models data.\n",
        "    \"\"\"\n",
        "\n",
        "    models_data = json.load(open(model_path))\n",
        "    return models_data\n",
        "\n",
        "\n",
        "def get_model_hash_from_path(model_path:str=\"./mdxc/weights/MDX23C-8KFFT-InstVoc_HQ/MDX23C-8KFFT-InstVoc_HQ.ckpt\")->str:\n",
        "    \"\"\"\n",
        "    Get the hash of the model from the specified model path.\n",
        "\n",
        "    Args:\n",
        "        model_path (str): The path to the model file. Default is \"./mdxc/weights/UVR-MDX-NET-Inst_1/UVR-MDX-NET-Inst_1.ckpt\".\n",
        "\n",
        "    Returns:\n",
        "        str: The hash of the model.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        with open(model_path, 'rb') as f:\n",
        "            f.seek(- 10000 * 1024, 2)\n",
        "            model_hash = hashlib.md5(f.read()).hexdigest()\n",
        "    except:\n",
        "        model_hash = hashlib.md5(open(model_path,'rb').read()).hexdigest()\n",
        "    \n",
        "    return model_hash\n",
        "\n",
        "\n",
        "def load_mdxc_model_data(models_data, model_hash, model_path=\"./mdxc/modelparams\"):\n",
        "    \"\"\"\n",
        "    Load the mdxc model data from the specified models data and model hash.\n",
        "\n",
        "    Args:\n",
        "        models_data (dict): The models data.\n",
        "        model_hash (str): The hash of the model.\n",
        "\n",
        "    Returns:\n",
        "        dict: The loaded model data.\n",
        "    \"\"\"\n",
        "\n",
        "    model_data_src = models_data[model_hash]\n",
        "    # if not \"config_yaml\" in model_data_src: return model_data_src\n",
        "    model_path = os.path.join(model_path, \"mdx_c_configs\", model_data_src['config_yaml'])\n",
        "    model_data = yaml.load(open(model_path), Loader=yaml.FullLoader)\n",
        "\n",
        "    model_data = ConfigDict(model_data)\n",
        "    \n",
        "    return model_data\n",
        "\n",
        "models_data = load_mdxc_models_data(model_path=\"mdxc/modelparams/model_data.json\")\n",
        "model_hash = get_model_hash_from_path(model_path=\"./mdxc/weights/MDX23C-8KFFT-InstVoc_HQ/MDX23C-8KFFT-InstVoc_HQ.ckpt\")\n",
        "\n",
        "model_data = models_data[model_hash]\n",
        "\n",
        "model_data = load_mdxc_model_data(models_data, model_hash, model_path=\"./mdxc/modelparams\")\n",
        "\n",
        "\n",
        "model_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_modle(model_path:str, model_data:ConfigDict, device:str='cuda'):\n",
        "    \"\"\"\n",
        "    Load the model from the given path and return the loaded model.\n",
        "\n",
        "    Args:\n",
        "        model_path (str): The path to the model file.\n",
        "        model_data (int): The model data of type ConfigDict.\n",
        "        device (str): The device to load the model on. Defaults to 'cuda'.\n",
        "\n",
        "    Returns:\n",
        "        model_run (function): The loaded model.\n",
        "\n",
        "    \"\"\"\n",
        "    model = TFC_TDF_net(model_data, device=device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
        "    model.to(device).eval()\n",
        "    return model\n",
        "\n",
        "\n",
        "model_run = load_modle(\"./mdxc/weights/MDX23C-8KFFT-InstVoc_HQ/MDX23C-8KFFT-InstVoc_HQ.ckpt\",\n",
        "                       model_data, device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2, 17822209)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "def rerun_mp3(audio_file:NDArray, sample_rate:int=44100):\n",
        "    \"\"\"\n",
        "    Load an audio file and return the audio data.\n",
        "\n",
        "    Parameters:\n",
        "        audio_file (str): The path to the audio file.\n",
        "        sample_rate (int, optional): The desired sample rate of the audio data. Default is 44100.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: The audio data as a numpy array.\n",
        "    \"\"\"\n",
        "    with audioread.audio_open(audio_file) as f:\n",
        "        track_length = int(f.duration)\n",
        "\n",
        "    return librosa.load(audio_file, duration=track_length, mono=False, sr=sample_rate)[0]\n",
        "\n",
        "def prepare_mix(mix):\n",
        "    \n",
        "    audio_path = mix\n",
        "\n",
        "    if not isinstance(mix, np.ndarray):\n",
        "        mix, sr = librosa.load(mix, mono=False, sr=44100)\n",
        "    else:\n",
        "        mix = mix.T\n",
        "\n",
        "    if isinstance(audio_path, str):\n",
        "        if not np.any(mix) and audio_path.endswith('.mp3'):\n",
        "            mix = rerun_mp3(audio_path)\n",
        "\n",
        "    if mix.ndim == 1:\n",
        "        mix = np.asfortranarray([mix,mix])\n",
        "\n",
        "    return mix\n",
        "\n",
        "\n",
        "\n",
        "audio_file = \"/Users/mohannadbarakat/Downloads/t.wav\"\n",
        "mix = prepare_mix(audio_file)\n",
        "mix.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pitch_fix(source, sr_pitched, org_mix, semitone_shift):\n",
        "    source = spec_utils.change_pitch_semitones(source, sr_pitched, semitone_shift=semitone_shift)[0]\n",
        "    source = spec_utils.match_array_shapes(source, org_mix)\n",
        "    return source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "segment_size = 256\n",
        "prams = {\n",
        "    'is_mdx_c_seg_def': False,\n",
        "    'segment_size': segment_size,\n",
        "    'batch_size': 1,\n",
        "    'overlap_mdx23': 8,\n",
        "    'semitone_shift': 0,\n",
        "    # 'mdx_segment_size': segment_size\n",
        "}\n",
        "\n",
        "\n",
        "def demix(mix, prams, model, model_data, device='cpu'):\n",
        "    sr_pitched = 441000\n",
        "    org_mix = mix\n",
        "    semitone_shift = prams['semitone_shift']\n",
        "    if  semitone_shift != 0:\n",
        "        mix, sr_pitched = spec_utils.change_pitch_semitones(mix, 44100, semitone_shift=-semitone_shift)\n",
        "\n",
        "    \n",
        "    mix = torch.tensor(mix, dtype=torch.float32)\n",
        "\n",
        "    try:\n",
        "        S = model.num_target_instruments\n",
        "    except Exception as e:\n",
        "        S = model.module.num_target_instruments\n",
        "\n",
        "    if prams['is_mdx_c_seg_def']:\n",
        "        mdx_segment_size = model_data.inference.dim_t  \n",
        "    else:\n",
        "        mdx_segment_size = prams['segment_size']\n",
        "    \n",
        "    batch_size = prams['batch_size']\n",
        "    chunk_size = model_data.audio.hop_length * (mdx_segment_size - 1)\n",
        "    overlap = prams['overlap_mdx23']\n",
        "\n",
        "    hop_size = chunk_size // overlap\n",
        "    mix_shape = mix.shape[1]\n",
        "    pad_size = hop_size - (mix_shape - chunk_size) % hop_size\n",
        "    mix = torch.cat([torch.zeros(2, chunk_size - hop_size), mix, torch.zeros(2, pad_size + chunk_size - hop_size)], 1)\n",
        "\n",
        "    chunks = mix.unfold(1, chunk_size, hop_size).transpose(0, 1)\n",
        "    batches = [chunks[i : i + batch_size] for i in range(0, len(chunks), batch_size)]\n",
        "    \n",
        "    X = torch.zeros(S, *mix.shape) if S > 1 else torch.zeros_like(mix)\n",
        "    X = X.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        cnt = 0\n",
        "        for batch in batches:\n",
        "            x = model(batch.to(device))\n",
        "            \n",
        "            for w in x:\n",
        "                X[..., cnt * hop_size : cnt * hop_size + chunk_size] += w\n",
        "                cnt += 1\n",
        "\n",
        "    estimated_sources = X[..., chunk_size - hop_size:-(pad_size + chunk_size - hop_size)] / overlap\n",
        "    del X\n",
        "    pitch_fix = lambda s:pitch_fix(s, sr_pitched, org_mix, semitone_shift)\n",
        "\n",
        "    if S > 1:\n",
        "        sources = {k: pitch_fix(v) if semitone_shift!=0 else v for k, v in zip(model_data.training.instruments, estimated_sources.cpu().detach().numpy())}\n",
        "        del estimated_sources   \n",
        "        return sources\n",
        "    \n",
        "    est_s = estimated_sources.cpu().detach().numpy()\n",
        "    del estimated_sources\n",
        "\n",
        "    if semitone_shift!=0:\n",
        "        return pitch_fix(est_s)  \n",
        "    else:\n",
        "        return est_s\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/mohannadbarakat/Downloads/ultimatevocalremover_api/venv/lib/python3.11/site-packages/torch/functional.py:660: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/SpectralOps.cpp:879.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n"
          ]
        }
      ],
      "source": [
        "stems = demix(mix, prams, model_run, model_data, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Vocals': array([[ 1.1634671e-06,  8.9558523e-07,  1.6635396e-06, ...,\n",
              "          4.0641185e-06,  3.4885775e-06, -7.1862928e-07],\n",
              "        [ 1.7248444e-06,  2.1618505e-06,  1.6717951e-06, ...,\n",
              "          2.0794473e-06,  3.8903972e-06,  1.0307468e-06]], dtype=float32),\n",
              " 'Instrumental': array([[-1.2724404e-06, -1.0384624e-06, -1.7787546e-06, ...,\n",
              "          2.2797427e-05,  3.2313128e-05,  8.8528031e-07],\n",
              "        [-1.8492387e-06, -2.1128867e-06, -1.8117911e-06, ...,\n",
              "         -1.4569440e-05,  1.9893885e-05, -1.0879556e-06]], dtype=float32)}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "from mdx.constants import  MDX_NET_FREQ_CUT\n",
        "\n",
        "def get_secondery_stems(source, mix, prams, device='cpu'):\n",
        "    mdx_net_cut = False\n",
        "\n",
        "    if (prams['primary_stem'] in MDX_NET_FREQ_CUT) and prams['is_match_frequency_pitch']:\n",
        "        mdx_net_cut = True\n",
        "\n",
        "    if mdx_net_cut:\n",
        "        raw_mix = demix(match_frequency_pitch(mix, prams), prams, device=device, is_match_mix=True)  \n",
        "    else:\n",
        "        match_frequency_pitch(mix, prams)\n",
        "\n",
        "    if prams['is_invert_spec']:\n",
        "        secondary_source = spec_utils.invert_stem(raw_mix, source) \n",
        "    else: \n",
        "        secondary_source = mix.T-source.T\n",
        "    \n",
        "    return secondary_source\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/16/2k15h2fx5vb9krn2ckrnjjbw0000gn/T/ipykernel_56294/1141985001.py:93: RuntimeWarning: invalid value encountered in divide\n",
            "  tar_waves = result / divider\n"
          ]
        }
      ],
      "source": [
        "second_stem = get_secondery_stems(stems, mix, prams, device='cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2, 17822209)"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def nparray_stem_to_dict(stems, second_stem, model_data):\n",
        "    if stems.shape[0] != 2:\n",
        "        stems = stems.T\n",
        "    if second_stem.shape[0] != 2:\n",
        "        second_stem = second_stem.T\n",
        "    return {\n",
        "        model_data['primary_stem'].lower(): stems,\n",
        "        secondary_stem(model_data['primary_stem']).lower(): second_stem\n",
        "    }\n",
        "\n",
        "\n",
        "dect_stems = nparray_stem_to_dict(stems, second_stem, model_data)\n",
        "\n",
        "dect_stems['vocals'].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_samplerate = 44100\n",
        "path = \"vocals.wav\"\n",
        "audiofile.write(path, dect_stems['vocals'], model_samplerate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "path = \"instrumental.wav\"\n",
        "audiofile.write(path, dect_stems['instrumental'], model_samplerate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
