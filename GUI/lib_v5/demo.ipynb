{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEN2Ei3D284H"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nkv0g0H0yNA4"
      },
      "outputs": [],
      "source": [
        "from vr_network import nets_new\n",
        "from vr_network import nets\n",
        "from vr_network import spec_utils\n",
        "\n",
        "import torch\n",
        "import audiofile\n",
        "from IPython.display import Audio, display\n",
        "import soundfile as sf\n",
        "import json \n",
        "import hashlib\n",
        "import librosa\n",
        "import numpy as np\n",
        "import audioread\n",
        "import platform\n",
        "from numpy.typing import NDArray\n",
        "from typing import Union\n",
        "import math, os\n",
        "\n",
        "if torch.cuda.is_available(): device = \"cuda\"\n",
        "elif torch.backends.mps.is_available(): device = torch.device(\"mps\")\n",
        "else: device = \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhWOPekI3IbK"
      },
      "source": [
        "# Main code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def load_vr_models_data(model_path:str=\"vr_network/modelparams/model_data.json\")->dict:\n",
        "    \"\"\"\n",
        "    Load the VR models data from the specified model path.\n",
        "\n",
        "    Args:\n",
        "        model_path (str): The path to the model data JSON file. Default is \"vr_network/modelparams/model_data.json\".\n",
        "\n",
        "    Returns:\n",
        "        dict: The loaded models data.\n",
        "    \"\"\"\n",
        "\n",
        "    models_data = json.load(open(model_path))\n",
        "    return models_data\n",
        "\n",
        "\n",
        "def get_model_hash_from_path(model_path:str=\"vr_network/weights/1_HP-UVR.pth\")->str:\n",
        "    \"\"\"\n",
        "    Get the hash of the model from the specified model path.\n",
        "\n",
        "    Args:\n",
        "        model_path (str): The path to the model file. Default is \"vr_network/weights/1_HP-UVR.pth\".\n",
        "\n",
        "    Returns:\n",
        "        str: The hash of the model.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        with open(model_path, 'rb') as f:\n",
        "            f.seek(- 10000 * 1024, 2)\n",
        "            model_hash = hashlib.md5(f.read()).hexdigest()\n",
        "    except:\n",
        "        model_hash = hashlib.md5(open(model_path,'rb').read()).hexdigest()\n",
        "    \n",
        "    return model_hash\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "N_BINS = 'n_bins'\n",
        "\n",
        "def int_keys(d):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        d (dict): The input dictionary.\n",
        "\n",
        "    Returns:\n",
        "        dict: A new dictionary with the keys converted to integers if they are numeric strings.\n",
        "\n",
        "    Example:\n",
        "        >>> d = {'1': 'one', '2': 'two', '3': 'three'}\n",
        "        >>> int_keys(d)\n",
        "        {1: 'one', 2: 'two', 3: 'three'}\n",
        "        \n",
        "    The int_keys function is a helper function defined outside the ModelParameters class. \n",
        "    It takes a dictionary d as input and returns a new dictionary r where the keys are converted \n",
        "    to integers if they are numeric strings.\n",
        "\n",
        "    Args:\n",
        "        d (dict): The input dictionary.\n",
        "\n",
        "    Returns:\n",
        "        dict: A new dictionary with the keys converted to integers if they are numeric strings.\n",
        "\n",
        "    Example:\n",
        "        >>> d = {'1': 'one', '2': 'two', '3': 'three'}\n",
        "        >>> int_keys(d)\n",
        "        {1: 'one', 2: 'two', 3: 'three'}\n",
        "\n",
        "    The purpose of this function is to ensure that the keys in the resulting dictionary \n",
        "    are integers instead of strings if they represent numeric values. This can be useful in cases where \n",
        "    the keys need to be used for numerical operations or comparisons.\n",
        "    \"\"\"\n",
        "    r = {}\n",
        "    for k, v in d:\n",
        "        if k.isdigit():\n",
        "            k = int(k)\n",
        "        r[k] = v\n",
        "    return r\n",
        "\n",
        "class ModelParameters(object):\n",
        "    \"\"\"\n",
        "    This class is used to store the parameters of the model. It reads the configuration file and stores the parameters in the instance.\n",
        "    \"\"\"\n",
        "    def __init__(self, config_path=''):\n",
        "        \"\"\"\n",
        "        Initializes an instance of ModelParameters.It reads the configuration file and stores the parameters in the instance.\n",
        "        all the parameters are stored in the self.param dictionary.\n",
        "\n",
        "        Args:\n",
        "            config_path (str): The path to the configuration file.\n",
        "\n",
        "        \"\"\"\n",
        "        with open(config_path, 'r') as f:\n",
        "                self.param = json.loads(f.read(), object_pairs_hook=int_keys)\n",
        "                \n",
        "        for k in ['mid_side', 'mid_side_b', 'mid_side_b2', 'stereo_w', 'stereo_n', 'reverse']:\n",
        "            if not k in self.param:\n",
        "                self.param[k] = False\n",
        "                \n",
        "        if N_BINS in self.param:\n",
        "            self.param['bins'] = self.param[N_BINS]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def get_capacity_and_vr_model(model_data)->tuple:\n",
        "    \"\"\"\n",
        "    Get the capacity and VR model from the specified model data.\n",
        "\n",
        "    Args:\n",
        "        model_data (dict): The model data.\n",
        "\n",
        "    Returns:\n",
        "        tuple: The VR model and capacity. (is_vr_51_model, model_capacity)\n",
        "    \n",
        "    The variable is_vr_51_model is a boolean flag that indicates whether the VR model is a 5.1 vocal remover model.\n",
        "    It is used to determine the capacity of the model. If is_vr_51_model is True, \n",
        "    then the model capacity is determined by the values of model_data[\"nout\"] and model_data[\"nout_lstm\"]. \n",
        "    Otherwise, the default model capacity is set to (32, 128).\n",
        "    \"\"\"\n",
        "\n",
        "   \n",
        "    is_vr_51_model = False\n",
        "    model_capacity = 32, 128\n",
        "\n",
        "    if \"nout\" in model_data.keys() and \"nout_lstm\" in model_data.keys():\n",
        "        model_capacity = model_data[\"nout\"], model_data[\"nout_lstm\"]\n",
        "        is_vr_51_model = True\n",
        "\n",
        "    return is_vr_51_model, model_capacity\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aWqJZA23Kbs"
      },
      "outputs": [],
      "source": [
        "\n",
        "def _load_model_with_hprams(model_path:str, mp:ModelParameters, \n",
        "                            is_vr_51_model:bool=False, model_capacity:tuple=(32, 128))->torch.nn.Module:\n",
        "    \"\"\"\n",
        "    Loads a model from the given model_path and returns the loaded model.\n",
        "\n",
        "    Parameters:\n",
        "        model_path (str): The path to the model file.\n",
        "        mp (ModelParameters): An instance of ModelParameters class.\n",
        "        is_vr_51_model (bool): Indicates whether the model is a VR 5.1 model.\n",
        "        model_capacity (tuple): A tuple representing the model capacity. Default is (32, 128).\n",
        "\n",
        "    Returns:\n",
        "        model_run: The loaded model.\n",
        "    \"\"\"\n",
        "    nn_arch_sizes = [\n",
        "        31191, # default\n",
        "        33966, 56817, 123821, 123812, 129605, 218409, 537238, 537227]\n",
        "    vr_5_1_models = [56817, 218409]\n",
        "    model_size = math.ceil(os.stat(model_path).st_size / 1024)\n",
        "    nn_arch_size = min(nn_arch_sizes, key=lambda x:abs(x-model_size))\n",
        "\n",
        "    if nn_arch_size in vr_5_1_models or is_vr_51_model:\n",
        "        model_run = nets_new.CascadedNet(mp.param['bins'] * 2, \n",
        "                                                nn_arch_size, \n",
        "                                                nout=model_capacity[0], \n",
        "                                                nout_lstm=model_capacity[1])\n",
        "        \n",
        "    else:\n",
        "        model_run = nets.determine_model_capacity(mp.param['bins'] * 2, nn_arch_size)\n",
        "                    \n",
        "    model_run.load_state_dict(torch.load(model_path, map_location='cpu')) \n",
        "    model_run.to(device) \n",
        "\n",
        "    return model_run\n",
        "\n",
        "models_data = load_vr_models_data()\n",
        "\n",
        "def get_secondary_stem(primary_stem:str)->str:\n",
        "    \"\"\"\n",
        "    Get the secondary stem from the given primary stem.\n",
        "\n",
        "    Args:\n",
        "        primary_stem (str): The primary stem.\n",
        "\n",
        "    Returns:\n",
        "        str: The secondary stem.\n",
        "    \"\"\"\n",
        "    stem_couples = {\n",
        "        'vocals': 'instruments',\n",
        "        'instruments': 'vocals',\n",
        "    }\n",
        "    \n",
        "    if primary_stem in stem_couples:\n",
        "        return stem_couples[primary_stem]\n",
        "    else:\n",
        "        if 'no' in primary_stem.lower(): return primary_stem.replace('no', '')\n",
        "        else: return 'no' + primary_stem\n",
        "\n",
        "def load_model(model_path:str)->tuple:\n",
        "    \"\"\"\n",
        "    Loads a model from the given model path.\n",
        "\n",
        "    Args:\n",
        "        model_path (str): The path to the model file.\n",
        "\n",
        "    Returns:\n",
        "        torch.nn.Module: The loaded model, \n",
        "        ModelParameters: Model parameters, \n",
        "        bool: A boolean flag indicating whether the model is a 5.1 vocal remover model,\n",
        "        dict(str, str): Model stems names (e.g. {\"primary_stem\": \"vocals\", \"secondary_stem\": \"instruments\"}).\n",
        "    \"\"\"\n",
        "    \n",
        "    model_hash = get_model_hash_from_path()\n",
        "    model_data = models_data[model_hash]\n",
        "\n",
        "    mp = f\"vr_network/modelparams/{model_data['vr_model_param']}.json\"\n",
        "\n",
        "    mp = ModelParameters(mp)\n",
        "\n",
        "    is_vr_51_model, model_capacity = get_capacity_and_vr_model(model_data)\n",
        "\n",
        "    model_run = _load_model_with_hprams(model_path, mp, is_vr_51_model, model_capacity)\n",
        "\n",
        "    primary_stem = model_data['primary_stem']\n",
        "    secondary_stem = get_secondary_stem(primary_stem)\n",
        "    strems = {\"primary_stem\":primary_stem, \"secondary_stem\":secondary_stem}\n",
        "    return model_run, mp, is_vr_51_model, strems\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "OPERATING_SYSTEM = platform.system()\n",
        "SYSTEM_PROC = platform.processor()\n",
        "SYSTEM_ARCH = platform.processor()\n",
        "ARM = 'arm'\n",
        "\n",
        "def rerun_mp3(audio_file:NDArray, sample_rate:int=44100):\n",
        "    \"\"\"\n",
        "    Load an audio file and return the audio data.\n",
        "\n",
        "    Parameters:\n",
        "        audio_file (str): The path to the audio file.\n",
        "        sample_rate (int, optional): The desired sample rate of the audio data. Default is 44100.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: The audio data as a numpy array.\n",
        "    \"\"\"\n",
        "    with audioread.audio_open(audio_file) as f:\n",
        "        track_length = int(f.duration)\n",
        "\n",
        "    return librosa.load(audio_file, duration=track_length, mono=False, sr=sample_rate)[0]\n",
        "\n",
        "def loading_mix(audio_file:NDArray, mp:ModelParameters, is_vr_51_model:bool, wav_type_set:str=\"PCM_U8\", high_end_process=None):\n",
        "    \"\"\"\n",
        "    Load and process the audio mix.\n",
        "\n",
        "    Parameters:\n",
        "        audio_file (str or numpy.ndarray): Path to the audio file or audio data as a numpy array.\n",
        "        mp (object): Object containing parameters for audio processing.\n",
        "        is_vr_51_model (bool): Flag indicating whether the model is a 5.1 vocal remover model.\n",
        "        wav_type_set (str): Subtype of the audio file. Options are ('PCM_U8', 'PCM_16', 'PCM_24', 'PCM_32', '32-bit Float', '64-bit Float') Default is \"PCM_U8\".\n",
        "        high_end_process (None or str): Type of high-end processing to be applied.\n",
        "\n",
        "    Returns:\n",
        "        X_spec (numpy.ndarray): Combined spectrogram of the audio mix.\n",
        "        input_high_end (numpy.ndarray or None): High-end portion of the audio mix.\n",
        "        input_high_end_h (int or None): Height of the high-end portion of the audio mix.\n",
        "    \"\"\"\n",
        "    X_wave, X_spec_s = {}, {}\n",
        "    \n",
        "    bands_n = len(mp.param['band'])\n",
        "    \n",
        "    audio_file = spec_utils.write_array_to_mem(audio_file, subtype=wav_type_set)\n",
        "    # save audio file to disk\n",
        "    is_mp3 = audio_file.endswith('.mp3') if isinstance(audio_file, str) else False\n",
        "\n",
        "    for d in range(bands_n, 0, -1):        \n",
        "        bp = mp.param['band'][d]\n",
        "    \n",
        "        if OPERATING_SYSTEM == 'Darwin':\n",
        "            wav_resolution = 'polyphase' if SYSTEM_PROC == ARM or ARM in SYSTEM_ARCH else bp['res_type']\n",
        "        else:\n",
        "            wav_resolution = bp['res_type']\n",
        "    \n",
        "        if d == bands_n: # high-end band\n",
        "            X_wave[d], _ = librosa.load(audio_file, sr=bp['sr'], mono=False, dtype=np.float32, res_type=wav_resolution)\n",
        "            X_spec_s[d] = spec_utils.wave_to_spectrogram(X_wave[d], bp['hl'], bp['n_fft'], mp, band=d, is_v51_model=is_vr_51_model)\n",
        "                \n",
        "            if not np.any(X_wave[d]) and is_mp3:\n",
        "                X_wave[d] = rerun_mp3(audio_file, bp['sr'])\n",
        "\n",
        "            if X_wave[d].ndim == 1:\n",
        "                X_wave[d] = np.asarray([X_wave[d], X_wave[d]])\n",
        "        else: # lower bands\n",
        "            X_wave[d] = librosa.resample(X_wave[d+1], orig_sr=mp.param['band'][d+1]['sr'], target_sr=bp['sr'], res_type=wav_resolution)\n",
        "            X_spec_s[d] = spec_utils.wave_to_spectrogram(X_wave[d], bp['hl'], bp['n_fft'], mp, band=d, is_v51_model=is_vr_51_model)\n",
        "\n",
        "        if d == bands_n and (high_end_process is not None):\n",
        "            input_high_end_h = (bp['n_fft']//2 - bp['crop_stop']) + (mp.param['pre_filter_stop'] - mp.param['pre_filter_start'])\n",
        "            input_high_end = X_spec_s[d][:, bp['n_fft']//2-input_high_end_h:bp['n_fft']//2, :]\n",
        "        else:\n",
        "            input_high_end_h = input_high_end = None\n",
        "            \n",
        "    X_spec = spec_utils.combine_spectrograms(X_spec_s, mp, is_v51_model=is_vr_51_model)\n",
        "    \n",
        "    del X_wave, X_spec_s, audio_file\n",
        "\n",
        "    return X_spec, input_high_end, input_high_end_h\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5FAGqjED3RM5"
      },
      "outputs": [],
      "source": [
        "from constants import NON_ACCOM_STEMS, VOCAL_STEM\n",
        "\n",
        "\n",
        "def _execute(X_mag_pad:NDArray, roi_size:int, model_run:torch.nn.Module, \n",
        "             batch_size:int, is_tta:bool, window_size:int)->NDArray:\n",
        "    \"\"\"\n",
        "    Executes the vocal removal algorithm on the given input spectrogram.\n",
        "\n",
        "    Args:\n",
        "        X_mag_pad (ndarray): Input spectrogram with padding.\n",
        "        roi_size (int): Size of the region of interest.\n",
        "        model_run (object): Instance of the model to run.\n",
        "        batch_size (int): Batch size for processing.\n",
        "        is_tta (bool): Flag indicating whether to use test-time augmentation.\n",
        "        window_size (int): Size of the sliding window.\n",
        "\n",
        "    Returns:\n",
        "        ndarray: Output mask representing the vocal component of the input spectrogram.\n",
        "    \"\"\"\n",
        "    X_dataset = []\n",
        "    patches = (X_mag_pad.shape[2] - 2 * model_run.offset) // roi_size\n",
        "    total_iterations = patches//batch_size if not is_tta else (patches//batch_size)*2\n",
        "    for i in range(patches):\n",
        "        start = i * roi_size\n",
        "        X_mag_window = X_mag_pad[:, :, start:start + window_size]\n",
        "        X_dataset.append(X_mag_window)\n",
        "\n",
        "    X_dataset = np.asarray(X_dataset)\n",
        "    model_run.eval()\n",
        "    with torch.no_grad():\n",
        "        mask = []\n",
        "        for i in range(0, patches, batch_size):\n",
        "            \n",
        "            X_batch = X_dataset[i: i + batch_size]\n",
        "            X_batch = torch.from_numpy(X_batch).to(device)\n",
        "            pred = model_run.predict_mask(X_batch)\n",
        "            \n",
        "            pred = pred.detach().cpu().numpy()\n",
        "            pred = np.concatenate(pred, axis=2)\n",
        "            mask.append(pred)\n",
        "        mask = np.concatenate(mask, axis=2)\n",
        "    return mask\n",
        "\n",
        "\n",
        "def postprocess(mask:NDArray, X_mag:NDArray, X_phase:NDArray, primary_stem:str, \n",
        "                aggressiveness:float=.05, post_process_threshold:Union[None, float]=None):\n",
        "    \"\"\"\n",
        "    Post-processes the mask to obtain the separated vocal and instrumental spectrograms.\n",
        "\n",
        "    Args:\n",
        "        mask (ndarray): The binary mask indicating the presence of vocals in the mixture.\n",
        "        X_mag (ndarray): The magnitude spectrogram of the mixture.\n",
        "        X_phase (ndarray): The phase spectrogram of the mixture.\n",
        "        primary_stem (str): The primary stem to be separated (e.g., 'vocals', 'drums', 'bass', etc.).\n",
        "        aggressiveness (float): The aggressiveness parameter for adjusting the mask. Value should be between -1 and 1. Default is 0.05 (best for vocals).\n",
        "        post_process_threshold (float, optional): The threshold for merging artifacts in the mask. If None, no post-processing is applied. Default is None.\n",
        "\n",
        "    Returns:\n",
        "        y_spec (ndarray): The separated vocal spectrogram.\n",
        "        v_spec (ndarray): The separated instrumental spectrogram.\n",
        "    \"\"\"\n",
        "    is_non_accom_stem = False\n",
        "    for stem in NON_ACCOM_STEMS:\n",
        "        if stem == primary_stem:\n",
        "            is_non_accom_stem = True\n",
        "    \n",
        "\n",
        "    mask = spec_utils.adjust_aggr(mask, is_non_accom_stem, aggressiveness)\n",
        "\n",
        "    if post_process_threshold is not None:\n",
        "        mask = spec_utils.merge_artifacts(mask, thres=post_process_threshold)\n",
        "\n",
        "    y_spec = mask * X_mag * np.exp(1.j * X_phase)\n",
        "    v_spec = (1 - mask) * X_mag * np.exp(1.j * X_phase)\n",
        "\n",
        "    return y_spec, v_spec\n",
        "    \n",
        "def inference_vr(model_run, X_spec:NDArray, aggressiveness:float=.05, \n",
        "                  window_size:int=512, is_tta:bool=False, batch_size:int=4, \n",
        "                  post_process_threshold:Union[None, float]=None, primary_stem:str=VOCAL_STEM)->tuple:\n",
        "    \"\"\"\n",
        "    Perform vocal removal inference on a given spectrogram.\n",
        "\n",
        "    Args:\n",
        "        model_run (object): The model to run.\n",
        "        X_spec (ndarray): The input spectrogram.\n",
        "        aggressiveness (float): The aggressiveness parameter for adjusting the mask. Value should be between -1 and 1. Default is 0.05 (best for vocals).\n",
        "        window_size (int): The size of the window for processing.\n",
        "        is_tta (bool): Flag indicating whether to use test-time augmentation.\n",
        "        batch_size (int): Batch size for processing.\n",
        "        post_process_threshold (float, optional): The threshold for merging artifacts in the mask. If None, no post-processing is applied. Default is None.\n",
        "        primary_stem (str): The primary stem to be separated (e.g., 'vocals', 'drums', 'bass', etc.).\n",
        "\n",
        "    Returns:\n",
        "        tuple: The separated vocal and instrumental spectrograms. (y_spec:NDarray, v_spec:NDarray)\n",
        "    \"\"\"\n",
        "\n",
        "    X_mag, X_phase = spec_utils.preprocess(X_spec)\n",
        "    n_frame = X_mag.shape[2]\n",
        "    pad_l, pad_r, roi_size = spec_utils.make_padding(n_frame, window_size, model_run.offset)\n",
        "    X_mag_pad = np.pad(X_mag, ((0, 0), (0, 0), (pad_l, pad_r)), mode='constant')\n",
        "    X_mag_pad /= X_mag_pad.max()\n",
        "    mask = _execute(X_mag_pad, roi_size, model_run, batch_size, is_tta, window_size)\n",
        "\n",
        "    if is_tta:\n",
        "        pad_l += roi_size // 2\n",
        "        pad_r += roi_size // 2\n",
        "        X_mag_pad = np.pad(X_mag, ((0, 0), (0, 0), (pad_l, pad_r)), mode='constant')\n",
        "        X_mag_pad /= X_mag_pad.max()\n",
        "        mask_tta = _execute(X_mag_pad, roi_size, model_run, batch_size, is_tta, window_size)\n",
        "        mask_tta = mask_tta[:, :, roi_size // 2:]\n",
        "        mask = (mask[:, :, :n_frame] + mask_tta[:, :, :n_frame]) * 0.5\n",
        "    else:\n",
        "        mask = mask[:, :, :n_frame]\n",
        "\n",
        "    y_spec, v_spec = postprocess(mask, X_mag, X_phase, primary_stem, aggressiveness, post_process_threshold)\n",
        "    \n",
        "    return y_spec, v_spec\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vkx3MQRE3hnG"
      },
      "outputs": [],
      "source": [
        "def convert_spec_to_wav(spec:NDArray, model_params:ModelParameters, is_5_1_model:bool,\n",
        "                         high_end_process:str, input_high_end:NDArray, input_high_end_h:bool)->NDArray:    \n",
        "    \"\"\"\n",
        "    Converts a spectrogram to a waveform.\n",
        "\n",
        "    Args:\n",
        "        spec (np.ndarray): The input spectrogram.\n",
        "        model_params (ModelParameters): The parameters of the model.\n",
        "        is_5_1_model (bool): Indicates whether the model is a 5.1 model.\n",
        "        high_end_process (str): The high-end processing method.\n",
        "        input_high_end (np.ndarray): The input high-end data.\n",
        "        input_high_end_h (bool): Indicates whether the input high-end data is available.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The converted waveform.\n",
        "    \"\"\"\n",
        "    if isinstance(high_end_process, str) and high_end_process.startswith('mirroring') and isinstance(input_high_end, np.ndarray) and input_high_end_h:        \n",
        "        input_high_end_ = spec_utils.mirroring(high_end_process, spec, input_high_end, model_params)\n",
        "        wav = spec_utils.combine_spectrogram_to_wave(spec, model_params, input_high_end_h, input_high_end_, is_5_1_model=is_5_1_model)       \n",
        "    else:\n",
        "        wav = spec_utils.combine_spectrogram_to_wave(spec, model_params, is_5_1_model=is_5_1_model)\n",
        "        \n",
        "    return wav\n",
        "\n",
        "def convert_audio_spec_to_wav(spec:NDArray, model_params:ModelParameters, is_5_1_model:bool, model_samplerate:int):\n",
        "    \"\"\"\n",
        "    Convert audio spectrogram to waveform.\n",
        "\n",
        "    Args:\n",
        "        spec (numpy.ndarray): Input audio spectrogram.\n",
        "        model_params (ModelParameters): Model parameters.\n",
        "        is_5_1_model (bool): Flag indicating if the model is a 5.1 model.\n",
        "        model_samplerate (int): Model sample rate.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Output audio waveform.\n",
        "    \"\"\"\n",
        "    res = convert_spec_to_wav(spec, model_params, is_5_1_model, high_end_process, input_high_end, input_high_end_h).T\n",
        "    if model_samplerate == 44100:\n",
        "        res = librosa.resample(res.T, orig_sr=model_samplerate, target_sr=44100).T\n",
        "    return res.T\n",
        "\n",
        "def get_audio(y_spec:NDArray, v_spec:NDArray, model_params:ModelParameters, normaliz:bool)->dict:\n",
        "    \"\"\"\n",
        "    Convert audio spectrograms to audio waveforms and normalize them.\n",
        "\n",
        "    Args:\n",
        "        y_spec (numpy.ndarray): Spectrogram of the primary source audio.\n",
        "        v_spec (numpy.ndarray): Spectrogram of the secondary source audio.\n",
        "        model_params (ModelParams): Parameters of the model.\n",
        "        normaliz (bool): Flag indicating whether to normalize the audio waveforms.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the primary source audio waveform and the secondary source audio waveform.\n",
        "    \"\"\"\n",
        "    model_samplerate = model_params.param['sr']\n",
        "    primary_source = convert_audio_spec_to_wav(y_spec, model_params, model_samplerate)\n",
        "    secondary_source = convert_audio_spec_to_wav(v_spec, model_params)\n",
        "    \n",
        "    primary_source = spec_utils.normalize(primary_source, normaliz)\n",
        "    secondary_source = spec_utils.normalize(secondary_source, normaliz)\n",
        "\n",
        "    return {\"primary_source\": primary_source, \n",
        "            \"secondary_source\": secondary_source}\n",
        "\n",
        "def rename_audio_res_dict(audio_res:dict, names:dict)->dict:\n",
        "    \"\"\"\n",
        "    Rename the keys of the audio results dictionary.\n",
        "\n",
        "    Args:\n",
        "        audio_res (dict): The audio results dictionary.\n",
        "        names (dict): A dictionary containing the new names for the audio sources.\n",
        "\n",
        "    Returns:\n",
        "        dict: The renamed audio results dictionary.\n",
        "    \"\"\"\n",
        "    primary_name = names[\"primary_name\"]\n",
        "    secondary_name = names[\"secondary_name\"]\n",
        "    audio_res = {primary_name: audio_res[\"primary_source\"], \n",
        "                 secondary_name: audio_res[\"secondary_source\"]}\n",
        "    return audio_res\n",
        "\n",
        "\n",
        "def get_audio_dict(y_spec:NDArray, v_spec:NDArray, names:dict, model_params:ModelParameters, normaliz:bool)->dict:\n",
        "    \"\"\"\n",
        "    Convert audio spectrograms to audio waveforms and normalize them.\n",
        "\n",
        "    Args:\n",
        "        y_spec (numpy.ndarray): Spectrogram of the primary source audio.\n",
        "        v_spec (numpy.ndarray): Spectrogram of the secondary source audio.\n",
        "        model_params (ModelParameters): Parameters of the model.\n",
        "        names (dict): A dictionary containing the new names for the audio sources.\n",
        "        normaliz (bool): Flag indicating whether to normalize the audio waveforms.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the primary source audio waveform and the secondary source audio waveform.\n",
        "    \"\"\"\n",
        "    audio_res = get_audio(y_spec, v_spec, model_params, normaliz)\n",
        "    audio_res = rename_audio_res_dict(audio_res, names)\n",
        "    return audio_res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# class VR():\n",
        "    \n",
        "#     def __init__(self, other_metadata:dict, name:str=\"1_HP-UVR.pth\", device=None, logger=None):\n",
        "#         super().__init__(VR, architecture=\"vr_network\", other_metadata=other_metadata)\n",
        "#         current_path = os.getcwd()\n",
        "#         self.model_path = os.path.join(current_path, \"src\", \"models_dir\", \"vr_network\", \"weights\", name) \n",
        "#         self.model = None\n",
        "#         self.sample_rate = self.model_api._samplerate\n",
        "  \n",
        "#     def predict(self, audio:npt.NDArray, sampling_rate:int)->dict:\n",
        "#         \"\"\"Separate the audio into its components\n",
        "\n",
        "#         Args:\n",
        "#             audio (np.array): audio data\n",
        "#             sampling_rate (int): sampling rate\n",
        "\n",
        "#         Returns:\n",
        "#             dict: separated audio\n",
        "#         \"\"\"\n",
        "#         origin, separated = self.model_api.separate_tensor(audio, sampling_rate)\n",
        "#         return {\"origin\":origin, \"separated\":separated}\n",
        "    \n",
        "#     def to(self, device:str):\n",
        "#         self.model_api.update_parameter(device=device)\n",
        "#         self.model_api.model.to(device)\n",
        "    \n",
        "#     @staticmethod\n",
        "#     def list_models()->dict:\n",
        "#         return list(models_json[\"demucs\"].keys())\n",
        "    \n",
        "#     def update_metadata(self, metadata:dict):\n",
        "#         self.model_api.update_parameter(**metadata)\n",
        "#         self.other_metadata.update(metadata)\n",
        "\n",
        "#     def predict_path(self, audio: str) -> dict:\n",
        "#         audio, sampling_rate = read(audio)\n",
        "#         audio = torch.tensor(audio, dtype=torch.float32)\n",
        "#         return self.predict(audio, sampling_rate)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# audio_file = \"/Users/mohannadbarakat/Downloads/onlymp3.to - مهرجان لو كنت قدي انزل تحدي الدخلاوية في امريكا فريق الاحلام الدخلاوية البوم سكة الادمان 2017-WeLg_g2Ccrg-192k-1707990140.mp3\"\n",
        "model_path = \"vr_network/weights/1_HP-UVR.pth\"\n",
        "audio_file = \"/Users/mohannadbarakat/Downloads/t.wav\"\n",
        "high_end_process = None\n",
        "wav_type_set = 'PCM_U8'\n",
        "window_size = 512\n",
        "post_process_threshold = None\n",
        "batch_size = 4 \n",
        "is_tta=False\n",
        "aggressiveness = .05\n",
        "\n",
        "model_run, mp, is_vr_51_model, strems  = load_model(model_path)\n",
        "\n",
        "\n",
        "aggressiveness = {'value': aggressiveness, \n",
        "                'split_bin': mp.param['band'][1]['crop_stop'], \n",
        "                'aggr_correction': mp.param.get('aggr_correction')}\n",
        "\n",
        "inp, input_high_end, input_high_end_h = loading_mix(audio_file, mp, is_vr_51_model, \n",
        "                                                    wav_type_set=wav_type_set, high_end_process=high_end_process)\n",
        "\n",
        "\n",
        "\n",
        "y_spec, v_spec = inference_vr(inp, aggressiveness=aggressiveness, window_size=window_size,\n",
        "                               model_run=model_run,is_tta=is_tta, batch_size=batch_size, \n",
        "                               post_process_threshold=post_process_threshold, primary_stem=strems[\"primary_stem\"])\n",
        "\n",
        "audio_res = get_audio_dict(y_spec=y_spec, v_spec=v_spec, strems=strems, mp=mp, normaliz=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
              "         3.42674331e-05,  1.99832780e-05,  1.80604391e-05],\n",
              "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
              "         2.06695778e-05, -1.62613936e-05,  1.00694547e-05]])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocals_path = \"vocals.wav\"\n",
        "\n",
        "audiofile.write(vocals_path, primary_source, model_samplerate)\n",
        "# sf.write(vocals_path, primary_source, 44100, format='mp3')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "insturemntal_path = \"insturemntas.wav\"\n",
        "audiofile.write(insturemntal_path, secondary_source, model_samplerate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "UTDkfvdL3UeT"
      },
      "outputs": [],
      "source": [
        "# display(Audio(\"vocals.mp3\", autoplay=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "oCUJwn1a3jQl"
      },
      "outputs": [],
      "source": [
        "# display(Audio(\"insturemntas.mp3\", autoplay=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
