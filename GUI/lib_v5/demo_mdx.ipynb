{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEN2Ei3D284H"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "nkv0g0H0yNA4"
      },
      "outputs": [],
      "source": [
        "from mdx.tfc_tdf_v3 import TFC_TDF_net, STFT\n",
        "from mdx.tfc_tdf_v3 import TFC_TDF_net, STFT\n",
        "import mdx.mdxnet as MdxnetSet\n",
        "from mdx import spec_utils\n",
        "from mdx.constants import secondary_stem\n",
        "import onnxruntime as ort\n",
        "from onnx import load\n",
        "from onnx2pytorch import ConvertModel\n",
        "\n",
        "import torch\n",
        "import audiofile\n",
        "from IPython.display import Audio, display\n",
        "import soundfile as sf\n",
        "import json \n",
        "import hashlib\n",
        "import librosa\n",
        "import numpy as np\n",
        "import audioread\n",
        "import platform\n",
        "from numpy.typing import NDArray\n",
        "from typing import Union\n",
        "import math, os\n",
        "\n",
        "if torch.cuda.is_available(): device = \"cuda\"\n",
        "elif torch.backends.mps.is_available(): device = torch.device(\"mps\")\n",
        "else: device = \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhWOPekI3IbK"
      },
      "source": [
        "# Main code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load model config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'compensate': 1.045,\n",
              " 'mdx_dim_f_set': 3072,\n",
              " 'mdx_dim_t_set': 8,\n",
              " 'mdx_n_fft_scale_set': 7680,\n",
              " 'primary_stem': 'Instrumental'}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "def load_mdx_models_data(model_path:str=\"mdx/modelparams/model_data.json\")->dict:\n",
        "    \"\"\"\n",
        "    Load the VR models data from the specified model path.\n",
        "\n",
        "    Args:\n",
        "        model_path (str): The path to the model data JSON file. Default is \"mdx/modelparams/model_data.json\".\n",
        "\n",
        "    Returns:\n",
        "        dict: The loaded models data.\n",
        "    \"\"\"\n",
        "\n",
        "    models_data = json.load(open(model_path))\n",
        "    return models_data\n",
        "\n",
        "\n",
        "def get_model_hash_from_path(model_path:str=\"./mdx/weights/UVR-MDX-NET-Inst_1/UVR-MDX-NET-Inst_1.onnx\")->str:\n",
        "    \"\"\"\n",
        "    Get the hash of the model from the specified model path.\n",
        "\n",
        "    Args:\n",
        "        model_path (str): The path to the model file. Default is \"./mdx/weights/UVR-MDX-NET-Inst_1/UVR-MDX-NET-Inst_1.onnx\".\n",
        "\n",
        "    Returns:\n",
        "        str: The hash of the model.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        with open(model_path, 'rb') as f:\n",
        "            f.seek(- 10000 * 1024, 2)\n",
        "            model_hash = hashlib.md5(f.read()).hexdigest()\n",
        "    except:\n",
        "        model_hash = hashlib.md5(open(model_path,'rb').read()).hexdigest()\n",
        "    \n",
        "    return model_hash\n",
        "\n",
        "\n",
        "models_data = load_mdx_models_data(model_path=\"mdx/modelparams/model_data.json\")\n",
        "model_hash = get_model_hash_from_path(model_path=\"./mdx/weights/UVR-MDX-NET-Inst_1/UVR-MDX-NET-Inst_1.onnx\")\n",
        "\n",
        "model_data = models_data[model_hash]\n",
        "\n",
        "\n",
        "model_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/mohannadbarakat/Downloads/ultimatevocalremover_api/venv/lib/python3.11/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:212.)\n",
            "  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n"
          ]
        }
      ],
      "source": [
        "def load_from_ckpt(model_path:str, device:str):\n",
        "    \"\"\"\n",
        "    Load a model from a checkpoint file and return the loaded model and its parameters.\n",
        "\n",
        "    Args:\n",
        "        model_path (str): The path to the checkpoint file.\n",
        "        device (str): The device to load the model on.\n",
        "\n",
        "    Returns:\n",
        "        model_run (torch.nn.Module): The loaded model.\n",
        "        (dim_c, hop) (tuple): The parameters of the model. (dim_c: int, hop: int)\n",
        "    \"\"\"\n",
        "    model_params = torch.load(model_path, map_location=lambda storage, loc: storage)['hyper_parameters']\n",
        "    dim_c, hop = model_params['dim_c'], model_params['hop_length']\n",
        "    separator = MdxnetSet.ConvTDFNet(**model_params)\n",
        "    model_run = separator.load_from_checkpoint(model_path).to(device).eval()\n",
        "    return model_run, (dim_c, hop)\n",
        "\n",
        "def device_to_ort_run_type(device:str):\n",
        "    \"\"\"\n",
        "    Converts the device name to the corresponding ONNX Runtime execution provider run type.\n",
        "\n",
        "    Args:\n",
        "        device (str): The device name. Valid options are 'cuda', 'cpu', and any other value.\n",
        "\n",
        "    Returns:\n",
        "        list: A list containing the ONNX Runtime execution provider run type.\n",
        "\n",
        "    \"\"\"\n",
        "    if device == 'cuda':\n",
        "        run_type = ['CUDAExecutionProvider']\n",
        "    else: #device == 'cpu':\n",
        "        run_type = ['CPUExecutionProvider']\n",
        "    # else:\n",
        "    #     run_type = ['DnnlExecutionProvider']\n",
        "    return run_type\n",
        "\n",
        "Dimf = (2048, 3072, 4096)\n",
        "Dim_t = (7, 8)\n",
        "Nfft = (4096, 5120, 6144, 7680, 8192, 16384)\n",
        "Volume = ('1.035', '1.08')\n",
        "\n",
        "\n",
        "def load_from_ort(model_path:str, device:str, segment_size:int, dim_t:int):\n",
        "\n",
        "    if segment_size == dim_t and  device == 'cpu':\n",
        "        ort_ = ort.InferenceSession(model_path, providers=device_to_ort_run_type(device))\n",
        "        model_run = lambda spek:ort_.run(None, {'input': spek.cpu().numpy()})[0]\n",
        "    else:\n",
        "        model_run = ConvertModel(load(model_path))\n",
        "        model_run.to(device).eval()\n",
        "    \n",
        "    return model_run, (4, 1024)\n",
        "\n",
        "def load_modle(model_path:str, device:str='cuda', segment_size:int=None, dim_t:int=None):\n",
        "    \"\"\"\n",
        "    Load the model from the given path and return the loaded model.\n",
        "\n",
        "    Args:\n",
        "        model_path (str): The path to the model file.\n",
        "        device (str): The device to load the model on. Defaults to 'cuda'.\n",
        "        segment_size (int): The segment size of the model. Defaults to None.\n",
        "        dim_t (int): The time dimension of the model. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        model_run (function): The loaded model.\n",
        "\n",
        "    \"\"\"\n",
        "    if model_path.endswith('.onnx'):\n",
        "        model_run = load_from_ort(model_path, device, segment_size, dim_t)\n",
        "    else:\n",
        "        model_run, (dim_c, hop) = load_from_ckpt(model_path, device)\n",
        "    return model_run\n",
        "\n",
        "# device = torch.de\n",
        "segment_size = 256\n",
        "dim_t = model_data['mdx_dim_t_set']\n",
        "model_run, (dim_c, hop) = load_modle('./mdx/weights/UVR-MDX-NET-Inst_1/UVR-MDX-NET-Inst_1.onnx', device, segment_size=segment_size, dim_t=dim_t)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2, 17822209)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "def rerun_mp3(audio_file:NDArray, sample_rate:int=44100):\n",
        "    \"\"\"\n",
        "    Load an audio file and return the audio data.\n",
        "\n",
        "    Parameters:\n",
        "        audio_file (str): The path to the audio file.\n",
        "        sample_rate (int, optional): The desired sample rate of the audio data. Default is 44100.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: The audio data as a numpy array.\n",
        "    \"\"\"\n",
        "    with audioread.audio_open(audio_file) as f:\n",
        "        track_length = int(f.duration)\n",
        "\n",
        "    return librosa.load(audio_file, duration=track_length, mono=False, sr=sample_rate)[0]\n",
        "\n",
        "def prepare_mix(mix):\n",
        "    \n",
        "    audio_path = mix\n",
        "\n",
        "    if not isinstance(mix, np.ndarray):\n",
        "        mix, sr = librosa.load(mix, mono=False, sr=44100)\n",
        "    else:\n",
        "        mix = mix.T\n",
        "\n",
        "    if isinstance(audio_path, str):\n",
        "        if not np.any(mix) and audio_path.endswith('.mp3'):\n",
        "            mix = rerun_mp3(audio_path)\n",
        "\n",
        "    if mix.ndim == 1:\n",
        "        mix = np.asfortranarray([mix,mix])\n",
        "\n",
        "    return mix\n",
        "\n",
        "\n",
        "\n",
        "audio_file = \"/Users/mohannadbarakat/Downloads/t.wav\"\n",
        "mix = prepare_mix(audio_file)\n",
        "mix.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialize_model_settings(n_fft:int, hop:int, dim_f:int, segment_size:int, device:str, **kwargs):\n",
        "    n_bins = n_fft//2+1\n",
        "    trim = n_fft//2\n",
        "    chunk_size = hop * (segment_size-1)\n",
        "    gen_size = chunk_size-2*trim\n",
        "    stft = STFT(n_fft, hop, dim_f, device)\n",
        "\n",
        "    return stft, n_bins, trim, chunk_size, gen_size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_model(mix, model_run, stft, adjust, denoise, device, is_match_mix):\n",
        "    \n",
        "    spek = stft(mix.to(device))*adjust\n",
        "    spek[:, :, :3, :] *= 0 \n",
        "\n",
        "    if is_match_mix:\n",
        "        spec_pred = spek.cpu().numpy()\n",
        "    else:\n",
        "        if denoise:\n",
        "            spec_pred = -model_run(-spek)*0.5+model_run(spek)*0.5  \n",
        "        else:\n",
        "            spec_pred = model_run(spek)\n",
        "\n",
        "    return stft.inverse(torch.tensor(spec_pred).to(device)).cpu().detach().numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pitch_fix(source, sr_pitched, org_mix, semitone_shift):\n",
        "    source = spec_utils.change_pitch_semitones(source, sr_pitched, semitone_shift=semitone_shift)[0]\n",
        "    source = spec_utils.match_array_shapes(source, org_mix)\n",
        "    return source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "prams = {\n",
        "    'n_fft': model_data['mdx_n_fft_scale_set'],\n",
        "    'hop': hop,\n",
        "    'dim_f': model_data['mdx_dim_f_set'],\n",
        "    'segment_size': segment_size,\n",
        "    'overlap': 0.75,\n",
        "    'mdx_batch_size': 1,\n",
        "    'semitone_shift': 0,\n",
        "    'compensate': model_data['compensate'],\n",
        "    'adjust': 1.08, \n",
        "    'denoise': False,\n",
        "    'is_invert_spec': False,\n",
        "    'primary_stem': model_data['primary_stem'],\n",
        "    'is_match_frequency_pitch': True,\n",
        "    'overlap_mdx': None\n",
        "}\n",
        "\n",
        "\n",
        "def demix(mix, prams, device='cpu', is_match_mix=False):\n",
        "    \n",
        "    semitone_shift = prams['semitone_shift']\n",
        "    overlap = prams['overlap']\n",
        "    adjust = prams['adjust']\n",
        "    denoise = prams['denoise']\n",
        "    org_mix = mix\n",
        "    tar_waves_ = []\n",
        "    stft, n_bins, trim, chunk_size, gen_size = initialize_model_settings(device=device, **prams)\n",
        "    \n",
        "\n",
        "    if is_match_mix:\n",
        "        chunk_size = prams['hop'] * (256-1)\n",
        "        overlap = 0.02\n",
        "    else:\n",
        "        chunk_size = chunk_size\n",
        "        overlap = prams['overlap_mdx']\n",
        "        \n",
        "        if prams['semitone_shift']!=0:\n",
        "            mix, sr_pitched = spec_utils.change_pitch_semitones(mix, 44100, semitone_shift=-prams['semitone_shift'])\n",
        "\n",
        "    \n",
        "    if semitone_shift:\n",
        "        mix, sr_pitched = spec_utils.change_pitch_semitones(mix, 44100, semitone_shift=-semitone_shift)\n",
        "\n",
        "    gen_size = chunk_size-2*trim\n",
        "\n",
        "    pad = gen_size + trim - ((mix.shape[-1]) % gen_size)\n",
        "    mixture = np.concatenate((np.zeros((2, trim), dtype='float32'), mix, np.zeros((2, pad), dtype='float32')), 1)\n",
        "\n",
        "    if overlap is None:\n",
        "        step = chunk_size - prams['n_fft']  \n",
        "    else: \n",
        "        step = int((1 - overlap) * chunk_size)\n",
        "\n",
        "    result = np.zeros((1, 2, mixture.shape[-1]), dtype=np.float32)\n",
        "    divider = np.zeros((1, 2, mixture.shape[-1]), dtype=np.float32)\n",
        "    total = 0\n",
        "    total_chunks = (mixture.shape[-1] + step - 1) // step\n",
        "\n",
        "    for i in range(0, mixture.shape[-1], step):\n",
        "        total += 1\n",
        "        start = i\n",
        "        end = min(i + chunk_size, mixture.shape[-1])\n",
        "\n",
        "        chunk_size_actual = end - start\n",
        "\n",
        "        if overlap == 0:\n",
        "            window = None\n",
        "        else:\n",
        "            window = np.hanning(chunk_size_actual)\n",
        "            window = np.tile(window[None, None, :], (1, 2, 1))\n",
        "\n",
        "        mix_part_ = mixture[:, start:end]\n",
        "        if end != i + chunk_size:\n",
        "            pad_size = (i + chunk_size) - end\n",
        "            mix_part_ = np.concatenate((mix_part_, np.zeros((2, pad_size), dtype='float32')), axis=-1)\n",
        "\n",
        "        mix_part = torch.tensor([mix_part_], dtype=torch.float32).to(device)\n",
        "        mix_waves = mix_part.split(prams['mdx_batch_size'])\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for mix_wave in mix_waves:\n",
        "\n",
        "                tar_waves = run_model(mix_wave, model_run, stft, adjust, denoise, device, is_match_mix=is_match_mix)\n",
        "                \n",
        "                if window is not None:\n",
        "                    tar_waves[..., :chunk_size_actual] *= window \n",
        "                    divider[..., start:end] += window\n",
        "                else:\n",
        "                    divider[..., start:end] += 1\n",
        "\n",
        "                result[..., start:end] += tar_waves[..., :end-start]\n",
        "        \n",
        "    tar_waves = result / divider\n",
        "    tar_waves_.append(tar_waves)\n",
        "\n",
        "    tar_waves_ = np.vstack(tar_waves_)[:, :, trim:-trim]\n",
        "    tar_waves = np.concatenate(tar_waves_, axis=-1)[:, :mix.shape[-1]]\n",
        "    \n",
        "    source = tar_waves[:,0:None]\n",
        "\n",
        "    if semitone_shift:\n",
        "        source = pitch_fix(source, sr_pitched, org_mix, semitone_shift)\n",
        "\n",
        "    source =  source*prams['compensate']\n",
        "\n",
        "    return source\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/16/2k15h2fx5vb9krn2ckrnjjbw0000gn/T/ipykernel_56294/3833157391.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return stft.inverse(torch.tensor(spec_pred).to(device)).cpu().detach().numpy()\n",
            "/var/folders/16/2k15h2fx5vb9krn2ckrnjjbw0000gn/T/ipykernel_56294/947450788.py:93: RuntimeWarning: invalid value encountered in divide\n",
            "  tar_waves = result / divider\n"
          ]
        }
      ],
      "source": [
        "stems = demix(mix, prams, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pitch_fix(source, sr_pitched, org_mix, semitone_shift):\n",
        "        source = spec_utils.change_pitch_semitones(source, sr_pitched, semitone_shift=semitone_shift)[0]\n",
        "        source = spec_utils.match_array_shapes(source, org_mix)\n",
        "        return source\n",
        "    \n",
        "def match_frequency_pitch(mix, prams):\n",
        "    source = mix\n",
        "    semitone_shift = prams['semitone_shift']\n",
        "    if prams['is_match_frequency_pitch'] and semitone_shift!=0:\n",
        "        source, sr_pitched = spec_utils.change_pitch_semitones(mix, 44100, semitone_shift=-semitone_shift)\n",
        "        source = pitch_fix(source, sr_pitched, mix, semitone_shift)\n",
        "\n",
        "    return source\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "from mdx.constants import  MDX_NET_FREQ_CUT\n",
        "\n",
        "def get_secondery_stems(source, mix, prams, device='cpu'):\n",
        "    mdx_net_cut = False\n",
        "\n",
        "    if (prams['primary_stem'] in MDX_NET_FREQ_CUT) and prams['is_match_frequency_pitch']:\n",
        "        mdx_net_cut = True\n",
        "\n",
        "    if mdx_net_cut:\n",
        "        raw_mix = demix(match_frequency_pitch(mix, prams), prams, device=device, is_match_mix=True)  \n",
        "    else:\n",
        "        match_frequency_pitch(mix, prams)\n",
        "\n",
        "    if prams['is_invert_spec']:\n",
        "        secondary_source = spec_utils.invert_stem(raw_mix, source) \n",
        "    else: \n",
        "        secondary_source = mix.T-source.T\n",
        "    \n",
        "    return secondary_source\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/16/2k15h2fx5vb9krn2ckrnjjbw0000gn/T/ipykernel_56294/1141985001.py:93: RuntimeWarning: invalid value encountered in divide\n",
            "  tar_waves = result / divider\n"
          ]
        }
      ],
      "source": [
        "second_stem = get_secondery_stems(stems, mix, prams, device='cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2, 17822209)"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def nparray_stem_to_dict(stems, second_stem, model_data):\n",
        "    if stems.shape[0] != 2:\n",
        "        stems = stems.T\n",
        "    if second_stem.shape[0] != 2:\n",
        "        second_stem = second_stem.T\n",
        "    return {\n",
        "        model_data['primary_stem'].lower(): stems,\n",
        "        secondary_stem(model_data['primary_stem']).lower(): second_stem\n",
        "    }\n",
        "\n",
        "\n",
        "dect_stems = nparray_stem_to_dict(stems, second_stem, model_data)\n",
        "\n",
        "dect_stems['vocals'].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_samplerate = 44100\n",
        "path = \"vocals.wav\"\n",
        "audiofile.write(path, dect_stems['vocals'], model_samplerate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "path = \"instrumental.wav\"\n",
        "audiofile.write(path, dect_stems['instrumental'], model_samplerate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
